---
title: "Final Exam"
author: "Juan Harrington"
date: "Apr 30, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load libraries
#library(tidyverse)
library(reshape2)
library(knitr)
library(ggplot2)
library(gridExtra)
library(e1071)
library(mgcv)
library(glmnet)
library(leaps)
library(RColorBrewer)

# seed for reproducibility
seed <- 314159

# References:
#https://github.com/har33sh/keystroke-dynamics
#https://rstudio-pubs-static.s3.amazonaws.com/35817_2552e05f1d4e4db8ba87b334101a43da.html
#https://onlinecourses.science.psu.edu/stat857/node/223
#https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/src/multivariateanalysis.html#multivariate-analysis
#https://github.com/DJedamski/School/blob/master/Wine%20Quality%20Code.R
#https://rstudio-pubs-static.s3.amazonaws.com/237448_25448d1a60d24e599e9531bf76c39f20.html
#https://www.r-bloggers.com/multilabel-classification-with-neuralnet-package/


#mixed effects longitudinal
#http://reports-archive.adm.cs.cmu.edu/anon/2012/CMU-CS-12-100.pdf
# convert column times to a Task (H: hold, DD: down, UD: up) and LetterKey (a.n, Shift.r.o) etc
# removed UD due to correlation
```

Using a Pattern Recognition approach to accurately predict.

## Step 1 - Load Data

```{r load_data,echo=F,include=F,warning=F,message=F}
# load data
known <- read.csv("~/_DSU-MSA/STAT702/Final Exam/known.csv")
unknown <- read.csv("~/_DSU-MSA/STAT702/Final Exam/unknown.csv")
known$X <- NULL # remove unneeded columns
```

## Step 2 - Explore data

Looking at the summary we can see that there is only two sessions, 7 and 8 (convert to factor).  Also, there are variables with negative values (time).  There are no missing values.  There are more than one individual per session.

```{r explore,echo=F,warning=F,message=F}
# explore
summary(known)

# set sessionIndex to factor
known$sessionIndex <- as.factor(known$sessionIndex)

# check for missing values
missing <- sapply(known, function(x) sum(is.na(x)))
kable(missing, col.names = c("Count"),
      caption = "Missing values")

# session counts
tbl <- with(known, table(subject, "Session"=sessionIndex))
ggplot(as.data.frame(tbl), aes(Session, Freq, fill = Session)) + 
  geom_col(position = "dodge") + 
  scale_fill_brewer(palette="PuBu") +  theme_bw() +
  ggtitle("Frequency of Sessions") + theme(plot.title = element_text(hjust = 0.5))
```

```{r plot,echo=F,warning=F,message=F}
# Density curves
# g1 <- ggplot(known, aes(x=H.period)) + geom_density() +
#       geom_vline(aes(xintercept=mean(H.period, na.rm=T)),   # Ignore NA values for mean
#                   color="purple", linetype="dashed", size=1)
# 
# g2 <- ggplot(known, aes(x=DD.period.t)) + geom_density() +
#       geom_vline(aes(xintercept=mean(DD.period.t, na.rm=T)),   # Ignore NA values for mean
#                   color="purple", linetype="dashed", size=1)
# 
# g3 <- ggplot(known, aes(x=UD.period.t)) + geom_density() +
#       geom_vline(aes(xintercept=mean(UD.period.t, na.rm=T)),   # Ignore NA values for mean
#                   color="purple", linetype="dashed", size=1)
# 
# g4 <- ggplot(known, aes(x=H.t)) + geom_density() +
#       geom_vline(aes(xintercept=mean(H.t, na.rm=T)),   # Ignore NA values for mean
#                   color="purple", linetype="dashed", size=1)
# 
# grid.arrange(g1, g2, g3, g4, ncol = 2)

# g5 <- ggplot(known, aes(x=DD.t.i)) + geom_density() +
#       geom_vline(aes(xintercept=mean(DD.t.i, na.rm=T)),   # Ignore NA values for mean
#                   color="purple", linetype="dashed", size=1)
# 
# g6 <- ggplot(known, aes(x=H.i)) + geom_density() +
#       geom_vline(aes(xintercept=mean(H.i, na.rm=T)),   # Ignore NA values for mean
#                   color="purple", linetype="dashed", size=1)
# 
# g7 <- ggplot(known, aes(x=DD.i.e)) + geom_density() +
#       geom_vline(aes(xintercept=mean(DD.i.e, na.rm=T)),   # Ignore NA values for mean
#                   color="purple", linetype="dashed", size=1)
# 
# g8 <- ggplot(known, aes(x=H.e)) + geom_density() +
#       geom_vline(aes(xintercept=mean(H.e, na.rm=T)),   # Ignore NA values for mean
#                   color="purple", linetype="dashed", size=1)
# 
# grid.arrange(g5, g6, g7, g8, ncol = 2)
```

## Density Plots

```{r scatter_plot,echo=F,warning=F,message=F}
## Pairwise scatter plots

# p1 <- ggplot(known, aes(x=DD.period.t, y=UD.period.t)) +
#   geom_point(shape=1)
# 
# grid.arrange(p1, ncol = 2)

# Reference: https://www.kaggle.com/notaapple/detailed-exploratory-data-analysis-using-r
# Retrieved: 04/01/2018

# plot functions
doPlots <- function(data_in, fun, ii, ncol=3) {
  pp <- list()
  for (i in ii) {
    p <- fun(data_in=data_in, i=i)
    pp <- c(pp, list(p))
  }
  do.call("grid.arrange", c(pp, ncol=ncol))
}

plotDen <- function(data_in, i){
  data <- data.frame(x=data_in[[i]], subject = data_in$subject)
  p <- ggplot(data= data) + geom_line(aes(x = x), stat = 'density', size = 1,alpha = 1.0) +
    xlab(paste0((colnames(data_in)[i]), '\n', 'Skewness: ',round(skewness(data_in[[i]], na.rm = TRUE), 2))) + theme_light() 
  return(p)
}

doPlots(known, fun = plotDen, ii = 3:7, ncol = 2)
doPlots(known, fun = plotDen, ii = 8:13, ncol = 2)
doPlots(known, fun = plotDen, ii = 14:19, ncol = 2)
doPlots(known, fun = plotDen, ii = 20:25, ncol = 2)
doPlots(known, fun = plotDen, ii = 26:31, ncol = 2)
doPlots(known, fun = plotDen, ii = 32:34, ncol = 2)
```

```{r cor,echo=F,warning=F,message=F}
# check for multicollinearity
# https://www.youtube.com/watch?v=QruEcbgfhzo
# corr <- cor(known[-1])
# eigen(cor(known[-1]))$values # wide range
# max(eigen(cor(known[-1]))$values)/min(eigen(cor(known[-1]))$values)
# kappa(cor(known[-1]),exact=T)
# 
# # display variables with high correlation
# d_cor <- as.matrix(cor(known[,4:ncol(known)]))
# d_cor_melt <- arrange(melt(d_cor), -abs(value))
# high_cor <- subset(d_cor_melt, value > .9 & Var1 != Var2)
# kable(high_cor, caption = "High correlation variables")

# Reference: https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/src/multivariateanalysis.html#multivariate-analysis
# Retrieved: 04/01/2018
mosthighlycorrelated <- function(mydataframe,numtoreport)
  {
     # find the correlations
     cormatrix <- cor(mydataframe)
     # set the correlations on the diagonal or lower triangle to zero,
     # so they will not be reported as the highest ones:
     diag(cormatrix) <- 0
     cormatrix[lower.tri(cormatrix)] <- 0
     # flatten the matrix into a dataframe for easy sorting
     fm <- as.data.frame(as.table(cormatrix))
     # assign human-friendly names
     names(fm) <- c("First.Variable", "Second.Variable","Correlation")
     # sort and print the top n correlations
     head(fm[order(abs(fm$Correlation),decreasing=T),],n=numtoreport)
}
#mosthighlycorrelated(known[2:33], 10)
kable(mosthighlycorrelated(known[3:33], 10), caption = "High correlation variables")

# 10 variables with high correlation
# remove them from models
cor_var <- c("UD.five.Shift.r", "UD.e.five", "UD.l.Return",
             "UD.Shift.r.o", "UD.period.t", "UD.n.l",
             "UD.o.a", "UD.i.e", "UD.t.i", "UD.a.n")

col_ix <- which(colnames(known) %in% cor_var)

#c("UD.five.Shift.r", "UD.e.five", "UD.l.Return",
#             "UD.Shift.r.o", "UD.period.t", "UD.n.l",
#             "UD.o.a", "UD.i.e", "UD.t.i", "UD.a.n")
```

## Step 3 - Split data into training set and test set.  Highly correlated variables are removed.

```{r split_train_test,echo=F,warning=F,message=F}
# set seed 
set.seed(seed)

# split data into training and test sets using 70/30 split
rows <- sample(nrow(known), nrow(known)*0.7)
known.1 <-  known[, -col_ix]
known.train <- known.1[rows,]
known.test <- known.1[-rows,]
```

## Feature selection using best subset selection

```{r echo=F}
# set seed 
set.seed(seed)

# predict function from chapter 6 labs
predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  xvars <- names(coefi)
  mat[,xvars]%*%coefi
}

regfit.full <- regsubsets(subject ~ ., data = known.1, nvmax = 23, method = "forward")
reg.summary <- summary(regfit.full)

# plot
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
# which.max(reg.summary$adjr2)
points(15,reg.summary$adjr2[15], col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
# which.min(reg.summary$cp )
points(13,reg.summary$cp [13],col="red",cex=2,pch=20)
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type='l')
# which.min(reg.summary$bic )
points(9,reg.summary$bic [9],col="red",cex=2,pch=20)

#coef(regfit.full, 15)
#reg.summary$which[15,]

subset.formula <- as.formula("subject ~ rep + H.period + DD.period.t + H.t
                             + DD.t.i + H.i + H.e + DD.e.five + H.five + H.Shift.r
                             + DD.shift.r.o + H.o + H.a + H.l + H.Return")
```

## Multinomial Regression

https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/

```{r}
# set seed 
set.seed(seed)

library(nnet)
mn.fit <- multinom(formula = subset.formula, data = known.train)
mn.pred <- predict(mn.fit, newdata = known.test, type="class")
mn.err <- round(mean(mn.pred != known.test$subject), 7)

## terrible error rate
```

## LASSO

```{r}
# set seed 
# set.seed(seed)
# 
# #https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
# 
# # create matrix for training set and test set
# train.mat <- model.matrix(subject ~ ., data = known.train)[,-1]
# test.mat <- model.matrix(subject ~ ., data = known.test)[,-1]
# 
# # define grid to cover range of lambda
# grid <- 10 ^ seq(10, -2, length = 100)
# 
# # fit using lasso and cross validation to get best lambda 
# lasso.cv <- cv.glmnet(train.mat, known.train$subject, 
#                       alpha = 1, lambda = grid, nfolds = 4,
#                       family = "multinomial", type.multinomial = "grouped",
#                       type.measure = "class", parallel = T)
# bestlam.lasso <- lasso.cv$lambda.min 
# 
# # plot lambda
# plot(lasso.cv)
# 
# # fit lasso regression using best lambda
# lasso.mod <- glmnet(train.mat, known.train$subject, 
#                     alpha = 1, lambda = bestlam.lasso,
#                     family = "multinomial", 
#                     type.multinomial = "grouped")
# 
# # predict
# pred.lasso <- predict(lasso.mod, newx = test.mat, 
#                       s = bestlam.lasso, type = "class")
# # calculate test error
# lasso.err <-  round(mean(pred.lasso != known.test$subject), 7)
# 
# 
# # non-zero coefficients that have been fit
# coef.lasso <- lasso.mod$beta[,1]
# predict(lasso.mod, type = "coefficients", s = bestlam.lasso)[1:18,]
# 
# # display non zero coefficient estimates
# kable(coef.lasso[coef.lasso != 0],
#       caption = "Non zero coefficient estimates")

```

## Support Vector Machine (SVM)

```{r svm,echo=F,warning=F,message=F}
# set seed 
set.seed(seed)

# fit svm
svm <- svm(subject ~ ., data = known.train)
#summary(svm)

# predict
svm.pred <- predict(svm, known.test)
svm.err <- round(mean(svm.pred != known.test$subject), 6)

# confusion matrix
#library(caret)
#caret::confusionMatrix(as.factor(svm.pred),as.factor(known.test$subject))

# assign error to table
known.svm.col.err <- cbind("SVM", svm.err)
known.errors <- rbind(known.svm.col.err)
```

```{r}
# k-fold
# set seed
set.seed(seed)

# Create 5 folds
folds <- sample(1:5, nrow(known), replace = T)

# Create empty data frame to store error rates
kn.err <- NULL

# perform 5-fold cross validation
for(i in 1:5){
  # fit logistic regression
  known.svm.cv <- svm(subject ~ .,
                      data = known[folds!=i,])
  
  # predict
  svm.predict <- predict(known.svm.cv, known[folds==i,])
  
  # calculate error and add to array
  kn.err[i] <- mean(svm.predict != known$subject[folds==i]) 
}


# display error rates
kable(data.frame(kn.err),
      caption = "Error Rates for 5-fold Cross-validation")

# error rate
known.svm.cv.error <- round(mean(kn.err), 7)
```

## LDA

```{r}
# set seed 
set.seed(seed)

library(MASS)
# fit LDA
lda.known <- lda(subject ~ ., data = known.train)
lda.pred.known <- predict(lda.known, known.test)
lda.err <- round(mean(lda.pred.known$class != known.test$subject), 6)

# plot LDA
plot(lda.pred.known$x[,1], lda.pred.known$x[,2])

# assign error to table
known.lda.col.err <- cbind("LDA", lda.err)
known.errors <- rbind(known.errors, known.lda.col.err)
```

## QDA

ERROR: Some group is too small for `qda`.

```{r}
# set seed 
# set.seed(seed)
# 
# library(MASS)
# # fit LDA
# qda.known <- qda(subject ~ ., data = known.train)
# qda.pred.known <- predict(qda.known, known.test)
# qda.err <- round(mean(qda.pred.known$class != known.test$subject), 6)
# 
# # plot LDA
# plot(qda.pred.known$x[,1], qda.pred.known$x[,2])
# 
# # assign error to table
# known.qda.col.err <- cbind("QDA", qda.err)
# known.errors <- rbind(known.errors, known.qda.col.err)
```

## KNN

```{r}
# set seed 
set.seed(seed)
```


## Boosting

```{r}
# set seed 
set.seed(seed)

# https://cran.r-project.org/web/packages/gbm/gbm.pdf
# https://rpubs.com/agz1117/MachineLearning_HW4
library(gbm)

# fit using boosting
boost.known <- gbm(subject ~ ., data = known.train, distribution = "multinomial",
                   n.trees = 100, shrinkage = .05, interaction.depth = 3,
                   cv.folds = 5)
summary(boost.known)

# predict
boost.pred <- predict(boost.known, newdata = known.test, 
                      n.trees = 100, type = "response")

# Reference: https://stackoverflow.com/questions/29454883/in-gbm-multinomial-dist-how-to-use-predict-to-get-categorical-output
# Retrieved: 04/06/2018
p.predSubj <- apply(boost.pred, 1, which.max)
subj.pred <- colnames(boost.pred)[p.predSubj]

# error rate
boost.err <- mean(subj.pred != known.test$subject)

# assign error to table
known.gbm.col.err <- cbind("Gradient Boosting", boost.err)
known.errors <- rbind(known.errors, known.gbm.col.err)
```

```{r}
#https://rstudio-pubs-static.s3.amazonaws.com/237448_25448d1a60d24e599e9531bf76c39f20.html **
# set seed 
set.seed(seed)
# 
# library(nnet)
# glm.fit <- multinom(subject ~ ., data = known.train, maxit = 1000, MaxNWts = 4000)
# glm_output <- summary(glm.fit)
# z <- glm_output$coefficients/glm_output$standard.errors
# p <- (1-pnorm(abs(z),0,1))*2 # using two-tailed z test
# print(p, digits =2)
# 
# # predict
# glm.pred <- predict(glm.fit, known.test)
# glm.err <- round(mean(glm.pred != known.test$subject), 6)


# library(gbm)
# gbm.fit <- gbm(subject ~ .,
#               data              = known.train,
#               distribution      = "multinomial",
#               n.tree            = 5000,
#               cv.folds          = 5,
#               interaction.depth = 2,
#               verbose = F)
# 
# 
# gbm.pred <- predict(gbm.fit, n.trees = 500,
#                     newdata = known.test, type = "response")
```


## Summary

```{r summary,echo=F,warning=F,message=F}
colnames(known.errors) <- c("Method", "VSA")
kable(known.errors, 
      caption = "Test Error rates for different methods")
```


##########################

```{r plotx,echo=F,warning=F,message=F}
# # plot key duration by subject?
# 
# ## Variable with high correlation
# #library(dplyr)
# #library(reshape2)
# res <- cor(known[,4:ncol(known)])
# round(res, 3)
# d_cor <- as.matrix(cor(known[,4:ncol(known)]))
# d_cor_melt <- arrange(melt(d_cor), -abs(value))
# subset(d_cor_melt, value > .9 & Var1 != Var2)
# 
# 
# 
# #known$pred <- pred
# 
# #known[pred != known$subject,]
# 
# 
# # validation set approach (cant due to subgrouping -subject,session,rep)
# # set seed
# set.seed(seed)
# # split into training and test
# rows <- sample(nrow(known), nrow(known)*0.6)
# known.train.lr <- known[rows, ]
# known.test.lr <- known[-rows, ]
# svm <- svm(subject ~ ., data = known.train.lr)
# summary(svm)
# pred <- predict(svm, known.test.lr)
# err <- round(mean(pred != known.test.lr$subject), 6)
# 
# ## glmnet
# xmat <- model.matrix(subject ~ ., data = known)[,-1]
# ymat <- known$subject
# 
# 
# # define grid to cover range of lambda
# grid <- 10 ^ seq(10, -2, length = 100)
# 
# # fit ridge regression using cross validation to find optimal lambda
# fit.ridge.cv <- cv.glmnet(x=xmat, y=ymat, 
#                           alpha = 0, lambda = grid, family="multinomial")
# bestlam.ridge <- fit.ridge.cv$lambda.min 
# 
# plot(fit.ridge.cv)
# 
# # fit ridge regression using best lambda
# college.ridge.mod <- glmnet(xmat, ymat,  family="multinomial",
#                             alpha = 0, lambda = bestlam.ridge)
# 
# # predict w chosen lambda
# pred.college.ridge <- predict(college.ridge.mod, 
#                               s = bestlam.ridge, 
#                               newx = xmat)
# 
# # calculate test error
# college.ridge.err <- round(mean(pred.college.ridge != known$subject), 6)
# 
# 
# 
# ##multinom
# library(nnet)
# set.seed(seed)
# known$subj2 <- relevel(known$subject, ref = "s002")
# multinomModel <- multinom(subj2 ~ ., data=known, MaxNWts = 8000) # multinom Model
# summary (multinomModel)
# 
# # gam
# set.seed(seed)
# library(leaps)
# # split data into training and test sets using 70/30 split
# rows <- sample(nrow(known), nrow(known)*0.7)
# known.train <- known[rows, -ix]
# known.test <- known[-rows, -ix]
# 
# # predict function from chapter 6 labs
# predict.regsubsets <- function(object, newdata, id, ...){
#   form <- as.formula(object$call[[2]])
#   mat <- model.matrix(form, newdata)
#   coefi <- coef(object, id=id)
#   xvars <- names(coefi)
#   mat[,xvars]%*%coefi
# }
# 
# # perform forward stepwise selection
# known.fit.fwd <- regsubsets(subject ~ ., data = known.train, 
#                               nvmax = 23, method = "forward")
# known.fit.fwd.summary <- summary(known.fit.fwd)
# 
# # predict and determine best model
# err.fwd <- rep(NA, ncol(known.train)-1)
# for(i in 1:(ncol(known.train)-1)) {
#   pred.fwd <- predict(known.fit.fwd, known.test, id=i)
#   err.fwd[i] <- mean((known.test$subject - pred.fwd)^2)
# }
# 
# # get selected variables
# kable(coef(college.fit.fwd, which.min(err.fwd)),
#            caption = "Coefficients in best subset selection")
# 
# 
# # fit gam on selected variables from (a) using training set
# college.fit.gam <- gam(subject ~ H.period + 
#                          s(H.t), 
#                        data = known.train)

# GBM?

# create_model <- function(trainData, target) {
#   set.seed(seed)
#   myglm <- glm(target ~ ., data = trainData, family = "binomial")
#   return(myglm) 
# }

#https://arxiv.org/ftp/arxiv/papers/1708/1708.00931.pdf
## look at comparison of holding key down to coming up
#keyduration <- releasetime - presstime
#keylatency <- presstime - releasetime
#normalized_duration <- keyduration / releasetime - presstime
#normalized_latency <- keylatency / releasetime - presstime
```

Use KNN, SVM, GBM, GMM (mixed model), LDA, QDA as models.

CMU Features
10 jey-down
10 key-uo
11 dwell times
=31 features
.tie5Roanl + Enter = 11 chars

https://www.youtube.com/watch?v=fDjKa7yWk1U

